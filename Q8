#STEP 1: Load Inbuilt IMDB Dataset
import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense
from tensorflow.keras.preprocessing.text import Tokenizer

# Load IMDB dataset (only reviews, labels not needed)
# Load IMDB dataset
(x_train, _), (x_test, _) = imdb.load_data(num_words=10000)

#Preprocess & Vectorize Text Data (Tokenization)
# Get word index
word_index = imdb.get_word_index()

# Reverse mapping
reverse_word_index = {v + 3: k for k, v in word_index.items()}
reverse_word_index[0] = "<PAD>"
reverse_word_index[1] = "<START>"
reverse_word_index[2] = "<UNK>"
reverse_word_index[3] = "<UNUSED>"

# Decode reviews (LIMITED SIZE)
def decode_review(seq):
    return " ".join([reverse_word_index.get(i, "<UNK>") for i in seq])

texts = [decode_review(seq) for seq in x_train[:2000]]

#Create Inputâ€“Output Sequences for Next-Word Prediction
tokenizer = Tokenizer(num_words=10000, oov_token="<OOV>")
tokenizer.fit_on_texts(texts)

sequences = []
max_seq_len = 30   # LIMIT SEQUENCE LENGTH

for text in texts:
    token_list = tokenizer.texts_to_sequences([text])[0]
    for i in range(1, min(len(token_list), max_seq_len)):
        sequences.append(token_list[:i+1])

#Convert Dataset into Numerical Padded Sequences
max_len = 30

sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')

X = sequences[:, :-1]
y = sequences[:, -1]

y = tf.keras.utils.to_categorical(y, num_classes=10000)

#Build Sequential RNN Model
model = Sequential([
    Embedding(input_dim=10000, output_dim=32, input_length=max_len-1),
    SimpleRNN(64),
    Dense(10000, activation='softmax')
])

model.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

model.summary()

#Train the Model
model.fit(
    X, y,
    epochs=3,
    batch_size=64,
    validation_split=0.2
)

#Evaluate Model (Loss & Accuracy)
loss, accuracy = model.evaluate(X, y, verbose=0)
print("Model Loss:", loss)
print("Model Accuracy:", accuracy)

#Test the Model (Sample Prediction)
def predict_next_word(text):
    token_list = tokenizer.texts_to_sequences([text])[0]
    token_list = pad_sequences([token_list], maxlen=max_len-1, padding='pre')
    predicted = np.argmax(model.predict(token_list), axis=-1)
    return tokenizer.index_word.get(predicted[0], "UNKNOWN")

#Predict Next Word
sample_text = "this movie was"
print("Input:", sample_text)
print("Predicted Next Word:", predict_next_word(sample_text))

#Accept User Input and Predict Next Word
while True:
    user_text = input("\nEnter text (or type 'exit'): ")
    if user_text.lower() == 'exit':
        break
    print("Predicted Next Word:", predict_next_word(user_text))
